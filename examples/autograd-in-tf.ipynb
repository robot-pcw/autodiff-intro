{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. autodiff in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=x^2, dy/dx=2x=6.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tensorflow中使用tf.GradientTape开启上下文管理器，并对Operations进行'监控'，用于自动微分。\n",
    "Trainable variables：会被自动监控;\n",
    "Tensors：需要手动监控，通过调用该上下文管理器的`watch` method;\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.activations as activations\n",
    "import tensorflow.keras.losses as losses\n",
    "import numpy as np\n",
    "\n",
    "# normal Tensor\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    dy_dx = g.gradient(y, x)\n",
    "    print(f\"y=x^2, dy/dx=2x={dy_dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dw=75.0\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(5.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = w ** 3 \n",
    "    dz_dw = tape.gradient(z, w)\n",
    "    print(f\"dz/dw={dz_dw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx=2x=10.0\n",
      "d2y/dx2=2.0\n"
     ]
    }
   ],
   "source": [
    " \"\"\"支持高阶导\"\"\"\n",
    "x = tf.Variable(5.0)\n",
    "with tf.GradientTape() as g:\n",
    "    with tf.GradientTape() as gg:\n",
    "        gg.watch(x)\n",
    "        y = x * x\n",
    "        dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n",
    "        print(f\"dy/dx=2x={dy_dx}\")\n",
    "    d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n",
    "    print(f\"d2y/dx2={d2y_dx2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. gradient in nn.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 dense (full connent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 5.]\n",
      " [10.]], shape=(2, 1), dtype=float32)\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "dy/dw=w^T=[[3.]\n",
      " [3.]]\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "dy/dx=[[2. 3.]\n",
      " [2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "y = x*w + b\n",
    "\"\"\"\n",
    "x = tf.Variable([[1.0, 1.0], [2.0, 2.0]])\n",
    "w = tf.Variable([[2.0,], [3.0,]])\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    y = tf.matmul(x, w)\n",
    "    print(y)\n",
    "    dy_dw = tape.gradient(y, w)\n",
    "    print(f\"dy/dw={dy_dw}\")\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(f\"dy/dx={dy_dx}\")\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x --max_pool_1d--> [[[5. 3. 7. 8. 9. 3.]]]\n",
      "dy/dx=\n",
      "[[[0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1.]\n",
      "  [1. 0. 1. 1. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "max pooling (1D)\n",
    "\"\"\"\n",
    "x = tf.Variable(\n",
    "    [\n",
    "      [ [1,3,4,2,5,2],\n",
    "        [4,2,5,6,8,3],\n",
    "        [5,3,7,8,9,3] ]\n",
    "    ],\n",
    "    dtype=tf.float32\n",
    ") # batch_size * seq_len * feature_dim\n",
    "max_pool = layers.MaxPooling1D(pool_size=3, strides=1, padding='valid')\n",
    "max_pool.build(input_shape=(1,6,3))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(max_pool.variables)\n",
    "    tape.watch(x)\n",
    "    y = max_pool(x)\n",
    "    print(f\"x --max_pool_1d--> {y}\")\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(f\"dy/dx=\\n{dy_dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 2 2 1]\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "max pooling层的梯度（dy/dx）可通过初始化一个形如inputs的全零矩阵，并将max取得的最大值的位置argmax对应的值置为1得到。\n",
    "如果pooling时的窗口不重叠的话，也可以按如下的方式使用argmax找到最大值位置并进行one-hot编码\n",
    "\"\"\"\n",
    "argmax_x = np.argmax(x[0], axis=0) \n",
    "print(argmax_x)\n",
    "one_hot_argmax_x = np.eye(x.shape[1])[argmax_x].T\n",
    "print(one_hot_argmax_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x --mean_pool_1d--> [[[3.3333333 2.6666667 5.3333335 5.3333335 7.3333335 2.6666667]]]\n",
      "dy/dx=\n",
      "[[[0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "  [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "  [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mean pooling (1D)\n",
    "\"\"\"\n",
    "x = tf.Variable(\n",
    "    [\n",
    "      [ [1,3,4,2,5,2],\n",
    "        [4,2,5,6,8,3],\n",
    "        [5,3,7,8,9,3] ]\n",
    "    ],\n",
    "    dtype=tf.float32\n",
    ") # batch_size * seq_len * feature_dim\n",
    "mean_pool = layers.AveragePooling1D(pool_size=3, strides=1, padding='valid')\n",
    "mean_pool.build(input_shape=(1,6,3))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(max_pool.variables)\n",
    "    tape.watch(x)\n",
    "    y = mean_pool(x)\n",
    "    print(f\"x --mean_pool_1d--> {y}\")\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(f\"dy/dx=\\n{dy_dx}\")\n",
    "\"\"\"\n",
    "和max pooling时不同，mean pooling池化窗口内的梯度是平均分配的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x --softmax--> \n",
      " [[0.3006096  0.332225   0.3671654 ]\n",
      " [0.32204348 0.32204348 0.35591307]\n",
      " [0.32214165 0.32537922 0.3524791 ]]\n",
      "dy/dx=\n",
      "[[1.7917728e-08 1.9802153e-08 2.1884762e-08]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "softmax\n",
    "\"\"\"\n",
    "# (batch_size, logit)=(2, 3)\n",
    "x = tf.Variable(\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [1,1,2],\n",
    "        [-0.3,-0.2,0.6],\n",
    "    ],\n",
    "    dtype=tf.float32\n",
    ")*0.1\n",
    "softmax = layers.Softmax(axis=-1)\n",
    "softmax.build(input_shape=(3,2))\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(softmax.variables)\n",
    "    tape.watch(x)\n",
    "    y = softmax(x)\n",
    "    print(f\"x --softmax--> \\n {y}\")\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(f\"dy/dx=\\n{dy_dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x --softmax--> \n",
      " [[0.21194156 0.21194156 0.57611686]\n",
      " [0.32204348 0.32204348 0.35591307]\n",
      " [0.33222038 0.33222038 0.33555925]]\n",
      "dy/dx=\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "softmax\n",
    "\"\"\"\n",
    "# (batch_size, logit)=(3, 3)\n",
    "x = tf.Variable(\n",
    "    [\n",
    "        [1, 1, 2],\n",
    "        [0.1, 0.1, 0.2],\n",
    "        [0.01, 0.01, 0.02],\n",
    "    ],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = activations.softmax(x)\n",
    "    print(f\"x --softmax--> \\n {y}\")\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(f\"dy/dx=\\n{dy_dx}\")\n",
    "\"\"\"\n",
    "梯度消失\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x --softmax_with_ce-->  [0.55144477 1.1330687  1.0919567 ]\n",
      "dz/dx=\n",
      "[[ 0.21194156  0.21194156 -0.4238831 ]\n",
      " [ 0.32204348 -0.6779565   0.35591307]\n",
      " [ 0.33222038  0.33222038 -0.66444075]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "softmax + cross entropy\n",
    "\"\"\"\n",
    "# (batch_size, logit)=(3, 3)\n",
    "x = tf.Variable(\n",
    "    [\n",
    "        [1, 1, 2],\n",
    "        [0.1, 0.1, 0.2],\n",
    "        [0.01, 0.01, 0.02],\n",
    "    ],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "t = tf.Variable(\n",
    "    [\n",
    "        [0., 0., 1.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 0., 1.],\n",
    "    ],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "ce = losses.CategoricalCrossentropy(reduction=losses.Reduction.NONE) # 先不进行reduce，各样本间独立计算\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = activations.softmax(x)\n",
    "    z = ce(t, y)\n",
    "    print(f\"x --softmax_with_ce-->  {z}\")\n",
    "    dz_dx = tape.gradient(z, x)\n",
    "    print(f\"dz/dx=\\n{dz_dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.21194156  0.21194156 -0.42388314]\n",
      " [ 0.32204348 -0.6779565   0.35591307]\n",
      " [ 0.33222038  0.33222038 -0.66444075]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "softmax + cross entropy ’挽救‘了梯度\n",
    "实际上 dz/dx = y - t\n",
    "\"\"\"\n",
    "print(y - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x --sigmoid--> \n",
      " [[0.7310586]\n",
      " [0.5249792]\n",
      " [0.5025   ]]\n",
      "dy/dx=\n",
      "[[0.19661193]\n",
      " [0.24937604]\n",
      " [0.24999376]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sigmoid\n",
    "\"\"\"\n",
    "# (batch_size, logit)=(3, 1)\n",
    "x = tf.Variable(\n",
    "    [\n",
    "        [1],\n",
    "        [0.1],\n",
    "        [0.01],\n",
    "    ],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = activations.sigmoid(x)\n",
    "    print(f\"x --sigmoid--> \\n {y}\")\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(f\"dy/dx=\\n{dy_dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x --relu--> \n",
      " [[1. ]\n",
      " [0.1]\n",
      " [0. ]]\n",
      "dy/dx=\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "relu\n",
    "\"\"\"\n",
    "x = tf.Variable(\n",
    "    [   [1],\n",
    "        [0.1],\n",
    "        [0.]\n",
    "    ],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = activations.relu(x)\n",
    "    print(f\"x --relu--> \\n {y}\")\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(f\"dy/dx=\\n{dy_dx}\")\n",
    "\"\"\"\n",
    "注意到零值点的梯度为0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
